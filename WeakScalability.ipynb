{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db2016e-e7cd-46d8-89f4-dbfca71eef9d",
   "metadata": {},
   "source": [
    "### Notebook used for gathering weak scalability results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a1f273-1899-4e64-a7b6-81596fa72af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hdf5(iterator):\n",
    "    import time\n",
    "    try:\n",
    "        import h5py\n",
    "        from io import BytesIO\n",
    "        print(\"h5py version:\", h5py.__version__)\n",
    "    except ImportError as e:\n",
    "        print(\"!!! [ERROR] Worker side has no h5py installed !!!\")\n",
    "        raise e\n",
    "    \"\"\" Parsing the HDF5 file on the worker side to extract artist_name and artist_hotttnesss \"\"\"\n",
    "    partition_start_time = time.time()  # record partition start time\n",
    "    \n",
    "    for row in iterator:\n",
    "        file_path = row[\"path\"]\n",
    "        binary_data = row[\"content\"] \n",
    "        try:\n",
    "            with h5py.File(BytesIO(binary_data), \"r\") as h5_file:\n",
    "                if \"metadata\" in h5_file and \"songs\" in h5_file[\"metadata\"]:\n",
    "                    songs_data = h5_file[\"metadata\"][\"songs\"][:]\n",
    "\n",
    "                    # get artist_name and artist_hotttnesss\n",
    "                    artist_name = songs_data[0][\"artist_name\"].decode() if isinstance(songs_data[0][\"artist_name\"], bytes) else str(songs_data[0][\"artist_name\"])\n",
    "                    artist_hotttnesss = float(songs_data[0][\"artist_hotttnesss\"]) if songs_data[0][\"artist_hotttnesss\"] != \"nan\" else None\n",
    "\n",
    "                    if artist_hotttnesss is not None and 0.0 <= artist_hotttnesss <= 1.0:\n",
    "                        yield Row(artist_name=artist_name, artist_hotttnesss=artist_hotttnesss)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            \n",
    "    partition_end_time = time.time()\n",
    "    print(f\"Partition executed in {partition_end_time - partition_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c861face-440e-4cfd-ac22-6df4fa7fe90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Set parameters \n",
    "number_of_workers = 1 # Current number of workers\n",
    "fraction_of_data = number_of_workers / 3  # take 1/3, 2/3, 3/3 sizes of data\n",
    "print(fraction_of_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31819733-5d1f-4104-b34b-22f583bd29f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5py version: 3.13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=====================================================>   (28 + 2) / 30]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|artist_name|avg_hotttnesss    |\n",
      "+-----------+------------------+\n",
      "|Coldplay   |0.9160532355308533|\n",
      "|Rihanna    |0.9082026481628418|\n",
      "|Usher      |0.8535031676292419|\n",
      "|Lil Wayne  |0.8434308171272278|\n",
      "|Maroon 5   |0.8433802723884583|\n",
      "+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    " # Import libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import avg, desc\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import pandas as pd\n",
    "from operator import add\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import h5py\n",
    "    from io import BytesIO\n",
    "    print(\"h5py version:\", h5py.__version__)\n",
    "except ImportError as e:\n",
    "    print(\"!!! [ERROR] Worker side has no h5py installed !!!\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    ".master(\"spark://192.168.2.130:7077\") \\\n",
    ".appName(\"Group10\") \\\n",
    ".config(\"spark.dynamicAllocation.enabled\", True) \\\n",
    ".config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True) \\\n",
    ".config(\"spark.shuffle.service.enabled\", False) \\\n",
    ".config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\") \\\n",
    ".config(\"spark.executor.cores\", 2) \\\n",
    ".config(\"spark.driver.port\",9999)\\\n",
    ".config(\"spark.blockManager.port\",10005)\\\n",
    ".config(\"spark.cores.max\", \"12\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "# record start time\n",
    "total_start_time = time.time()\n",
    "\n",
    "# read all file paths on HDFS\n",
    "hdfs_base_path = \"hdfs://192.168.2.130:9000/data/MillionSongSubset/\"\n",
    "df_files = spark_session.read.format(\"binaryFile\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(hdfs_base_path) \\\n",
    "    .select(\"path\", \"content\")\n",
    "\n",
    "if 0 < fraction_of_data < 1:\n",
    "    # Sample a fraction of data\n",
    "    df_files = df_files.sample(fraction=fraction_of_data)\n",
    "\n",
    "# limit the number of worker tasks to prevent overloading of resources\n",
    "MAX_PARTITIONS = 30\n",
    "df_files = df_files.repartition(MAX_PARTITIONS)\n",
    "\n",
    "# parallel processing of HDF5 files\n",
    "rdd_parsed = df_files.rdd.mapPartitions(parse_hdf5)\n",
    "\n",
    "# convert to DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"artist_name\", StringType(), True),\n",
    "    StructField(\"artist_hotttnesss\", FloatType(), True)])\n",
    "df_songs = spark_session.createDataFrame(rdd_parsed, schema=schema)\n",
    "\n",
    "# calculate avg artist_hotttnesss get Top 5\n",
    "start_time = time.time() # record aggregation start time\n",
    "\n",
    "df_songs.groupBy(\"artist_name\") \\\n",
    "    .agg(avg(\"artist_hotttnesss\").alias(\"avg_hotttnesss\")) \\\n",
    "    .orderBy(\"avg_hotttnesss\", ascending=False) \\\n",
    "    .show(5, truncate=False)\n",
    "\n",
    "end_time = time.time()  # record end time\n",
    "\n",
    "# Record the total time of Spark task\n",
    "total_end_time = time.time()\n",
    "\n",
    "# Calculate execution time for aggregation\n",
    "aggregation_time = end_time - start_time\n",
    "\n",
    "# Calculate execution time for total spark job\n",
    "total_time = total_end_time - total_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "475d2200-40fa-47cd-8e5c-537f01688157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c6968c-245d-4a2f-8495-f615c71719a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.260109901428223"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add results to csv file\n",
    "aggregation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3027eac8-5de9-437d-af2f-a8b38e286d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.37993597984314"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08aebc8d-7d5c-445e-b33d-2d75439a0b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Create dataframe with new results\n",
    "results = pd.DataFrame([{\"aggregation_time\": aggregation_time,\n",
    "                        \"total_time\": total_time,\n",
    "                        \"num_of_workers\": number_of_workers,\n",
    "                        \"fraction_of_data\": fraction_of_data}])\n",
    "# Define the filename where metrics will be saved\n",
    "filename = \"weak_scalability.csv\"\n",
    "if os.path.exists(filename):\n",
    "    # Read csv file\n",
    "    weak_df = pd.read_csv(filename)\n",
    "\n",
    "    # remove rows with same number of nodes\n",
    "    weak_df = weak_df[weak_df[\"num_of_workers\"] != number_of_workers]\n",
    "    \n",
    "    # Append new results\n",
    "    weak_df = pd.concat([weak_df, results], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    weak_df = results\n",
    "\n",
    "# Write to /overwrite csv file\n",
    "weak_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a49682-085f-44ee-8336-d1f1d54bddc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
