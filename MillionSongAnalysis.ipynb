{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cdc4ab6-df19-4be8-a031-93bb48ebd409",
   "metadata": {},
   "source": [
    "### 1. Start Spark Session & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2b636f-bf23-4dfa-b6bd-46481753f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0015d7a-d0f6-4b0e-8536-e7949c400f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60cbfc6-3eeb-4d1f-bbab-690696a7dad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/10 21:30:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import avg, desc\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import pandas as pd\n",
    "from operator import add\n",
    "import time\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "    .master(\"spark://192.168.2.130:7077\") \\\n",
    "    .appName(\"Group10\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True) \\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True) \\\n",
    "    .config(\"spark.shuffle.service.enabled\", False) \\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\") \\\n",
    "    .config(\"spark.executor.cores\", 2) \\\n",
    "    .config(\"spark.driver.port\",9999)\\\n",
    "    .config(\"spark.blockManager.port\",10005)\\\n",
    "    .config(\"spark.cores.max\", \"12\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c200974b-0167-4db0-977c-ef9601ffff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7857e9a2-1a67-457a-a547-d21fee218249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5py version: 3.13.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import h5py\n",
    "    from io import BytesIO\n",
    "    print(\"h5py version:\", h5py.__version__)\n",
    "except ImportError as e:\n",
    "    print(\"!!! [ERROR] Worker side has no h5py installed !!!\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "065fa8e5-500a-49a8-a7cd-da714e740682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # recognize to file path\n",
    "# # df = spark_session.read.format(\"binaryFile\").load(\"hdfs://192.168.2.130:9000/data/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5\")\n",
    "# df = spark_session.read.format(\"binaryFile\").load(\"hdfs://192.168.2.130:9000/data/MillionSongSubset/A/N/F/TRANFRL128F931CF30.h5\")\n",
    "# # df.show(5, False)  # have a look at the dataset\n",
    "# df.select(\"path\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f2868-e431-4396-b7e8-f75ffa025434",
   "metadata": {},
   "source": [
    "### 2. Check a single dataset to get the overall information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e69102c-7c6c-4236-be0e-90bfa6d8a08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+------+\n",
      "|path                                                                        |length|\n",
      "+----------------------------------------------------------------------------+------+\n",
      "|hdfs://192.168.2.130:9000/data/MillionSongSubset/A/F/I/TRAFINB128F426E2F1.h5|165602|\n",
      "+----------------------------------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get file path\n",
    "df = spark_session.read.format(\"binaryFile\").load(\"hdfs://192.168.2.130:9000/data/MillionSongSubset/A/F/I/TRAFINB128F426E2F1.h5\")\n",
    "\n",
    "# print file length to check non+null\n",
    "df.select(\"path\", \"length\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a2e0dc5-1a6d-4601-a8cf-f07e83fd2eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 contains keys: ['analysis', 'metadata', 'musicbrainz']\n"
     ]
    }
   ],
   "source": [
    "# get binary file\n",
    "binary_data = df.select(\"content\").collect()[0][0]  \n",
    "\n",
    "# parsing HDF5 file\n",
    "with h5py.File(BytesIO(binary_data), \"r\") as h5_file:\n",
    "    print(\"HDF5 contains keys:\", list(h5_file.keys()))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d4d2b0-4e78-4f51-8085-e15570b8729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata keys: ['artist_terms', 'artist_terms_freq', 'artist_terms_weight', 'similar_artists', 'songs']\n"
     ]
    }
   ],
   "source": [
    "# retrive metadata \n",
    "with h5py.File(BytesIO(binary_data), \"r\") as h5_file:\n",
    "    if \"metadata\" in h5_file:\n",
    "        metadata_keys = list(h5_file[\"metadata\"].keys())  # get metadata\n",
    "        print(\"Metadata keys:\", metadata_keys)\n",
    "    else:\n",
    "        print(\"No 'metadata' found in the HDF5 file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e503a77-8634-4415-a7e3-2a67db991419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in metadata['songs']: ('analyzer_version', 'artist_7digitalid', 'artist_familiarity', 'artist_hotttnesss', 'artist_id', 'artist_latitude', 'artist_location', 'artist_longitude', 'artist_mbid', 'artist_name', 'artist_playmeid', 'genre', 'idx_artist_terms', 'idx_similar_artists', 'release', 'release_7digitalid', 'song_hotttnesss', 'song_id', 'title', 'track_7digitalid')\n",
      "analyzer_version: b''\n",
      "artist_7digitalid: 71484\n",
      "artist_familiarity: 0.4059239066513581\n",
      "artist_hotttnesss: 0.281396284444565\n",
      "artist_id: b'AR15YLD1187FB3D4DD'\n",
      "artist_latitude: 55.67631\n",
      "artist_location: b'Copenhagen, Denmark'\n",
      "artist_longitude: 12.56935\n",
      "artist_mbid: b'27fc27ae-3d8d-40cd-b3cf-ed2541a2321d'\n",
      "artist_name: b'John Tchicai'\n",
      "artist_playmeid: 56888\n",
      "genre: b''\n",
      "idx_artist_terms: 0\n",
      "idx_similar_artists: 0\n",
      "release: b'Hymn To Sophia (Hymne Til Sofia)'\n",
      "release_7digitalid: 91162\n",
      "song_hotttnesss: 0.0\n",
      "song_id: b'SOEULPB12A8C136DDF'\n",
      "title: b'Musica Sacra Nova: Morgen I Frydenlund'\n",
      "track_7digitalid: 965262\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(BytesIO(binary_data), \"r\") as h5_file:\n",
    "    if \"metadata\" in h5_file and \"songs\" in h5_file[\"metadata\"]:\n",
    "        songs_data = h5_file[\"metadata\"][\"songs\"][:]  # get Dataset\n",
    "        \n",
    "        # get features\n",
    "        feature_names = songs_data.dtype.names  \n",
    "        \n",
    "        print(\"Features in metadata['songs']:\", feature_names)  \n",
    "        \n",
    "        # check first data\n",
    "        first_entry = songs_data[0]\n",
    "        \n",
    "        # get feature and value\n",
    "        for name, value in zip(feature_names, first_entry):\n",
    "            print(f\"{name}: {value}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'songs' data found in metadata.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2a5fcaf-a46d-4b01-9d8c-6bb12ddf8a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song Title: Musica Sacra Nova: Morgen I Frydenlund, Artist: John Tchicai\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(BytesIO(binary_data), \"r\") as h5_file:\n",
    "    if \"metadata\" in h5_file and \"songs\" in h5_file[\"metadata\"]:\n",
    "        songs_data = h5_file[\"metadata\"][\"songs\"][:]\n",
    "\n",
    "        # get title and artist_name\n",
    "        song_title = songs_data[0][\"title\"].decode() if isinstance(songs_data[0][\"title\"], bytes) else str(songs_data[0][\"title\"])\n",
    "        artist_name = songs_data[0][\"artist_name\"].decode() if isinstance(songs_data[0][\"artist_name\"], bytes) else str(songs_data[0][\"artist_name\"])\n",
    "\n",
    "        print(f\"Song Title: {song_title}, Artist: {artist_name}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'songs' data found in metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2edafe-c707-478f-83fe-e62225de27aa",
   "metadata": {},
   "source": [
    "### 3. Top 5 Artists by Average artist hottness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e77e63b-57a5-4c79-b1a0-51407c3a5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record start time\n",
    "total_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87ce44c5-3acb-48a9-955f-d21cb8e5d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all file paths on HDFS\n",
    "hdfs_base_path = \"hdfs://192.168.2.130:9000/data/MillionSongSubset/\"\n",
    "df_files = spark_session.read.format(\"binaryFile\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(hdfs_base_path) \\\n",
    "    .select(\"path\", \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e06a344-3706-4648-9c65-c16502ba60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the number of worker tasks to prevent overloading of resources\n",
    "MAX_PARTITIONS = 30\n",
    "df_files = df_files.repartition(MAX_PARTITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fdec81b-5611-4321-952c-2a5e9e2f0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse HDF5 \n",
    "def parse_hdf5(iterator):\n",
    "    \"\"\" Parsing the HDF5 file on the worker side to extract artist_name and artist_hotttnesss \"\"\"\n",
    "    partition_start_time = time.time()  # record partition start time\n",
    "    \n",
    "    for row in iterator:\n",
    "        file_path = row[\"path\"]\n",
    "        binary_data = row[\"content\"] \n",
    "        try:\n",
    "            with h5py.File(BytesIO(binary_data), \"r\") as h5_file:\n",
    "                if \"metadata\" in h5_file and \"songs\" in h5_file[\"metadata\"]:\n",
    "                    songs_data = h5_file[\"metadata\"][\"songs\"][:]\n",
    "\n",
    "                    # get artist_name and artist_hotttnesss\n",
    "                    artist_name = songs_data[0][\"artist_name\"].decode() if isinstance(songs_data[0][\"artist_name\"], bytes) else str(songs_data[0][\"artist_name\"])\n",
    "                    artist_hotttnesss = float(songs_data[0][\"artist_hotttnesss\"]) if songs_data[0][\"artist_hotttnesss\"] != \"nan\" else None\n",
    "\n",
    "                    if artist_hotttnesss is not None:\n",
    "                        yield Row(artist_name=artist_name, artist_hotttnesss=artist_hotttnesss)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            \n",
    "    partition_end_time = time.time()\n",
    "    print(f\"Partition executed in {partition_end_time - partition_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aaa8115-d31b-4388-99c0-293bafbe5ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:======================================================>(327 + 1) / 328]"
     ]
    }
   ],
   "source": [
    "# parallel processing of HDF5 files\n",
    "rdd_parsed = df_files.rdd.mapPartitions(parse_hdf5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bd2d54d-871b-4083-911d-f72152808552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"artist_name\", StringType(), True),\n",
    "    StructField(\"artist_hotttnesss\", FloatType(), True)\n",
    "])\n",
    "df_songs = spark_session.createDataFrame(rdd_parsed, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f9687d0-3b89-47ab-aac3-a4edd36404f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================================================> (29 + 1) / 30]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------------------+\n",
      "|artist_name                       |avg_hotttnesss    |\n",
      "+----------------------------------+------------------+\n",
      "|Kanye West                        |1.0825026035308838|\n",
      "|Kanye West / Consequence / Cam'Ron|1.0825026035308838|\n",
      "|Kanye West / Lupe Fiasco          |1.0825026035308838|\n",
      "|Kanye West / Adam Levine          |1.0825026035308838|\n",
      "|Daft Punk                         |1.021255612373352 |\n",
      "+----------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# calculate avg artist_hotttnesss get Top 5\n",
    "start_time = time.time() # record aggregation start time\n",
    "\n",
    "df_songs.groupBy(\"artist_name\") \\\n",
    "    .agg(avg(\"artist_hotttnesss\").alias(\"avg_hotttnesss\")) \\\n",
    "    .orderBy(\"avg_hotttnesss\", ascending=False) \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a60283f-fd70-4385-8a5d-11be67f1f3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation took 15.48 seconds\n",
      "Total Spark job executed in 40.60 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()  # record end time\n",
    "print(f\"Aggregation took {end_time - start_time:.2f} seconds\")\n",
    "# Record the total time of Spark task\n",
    "total_end_time = time.time()\n",
    "print(f\"Total Spark job executed in {total_end_time - total_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "328e4132-eeda-447d-867c-45db867a2624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop Spark session\n",
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ef30b-209d-4ca6-9911-232dc0aa3ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
